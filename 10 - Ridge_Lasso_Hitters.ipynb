{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge,Lasso\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Hitters_Data.csv').dropna()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df.Salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(np.log(df.Salary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(df[['League', 'Division', 'NewLeague']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log(df.Salary)\n",
    "\n",
    "# Drop the column with the independent variable (Salary), and columns for which we created dummy variables\n",
    "X_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis = 1).astype('float64')\n",
    "\n",
    "# Define the feature set X.\n",
    "X = pd.concat([X_, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis = 1)\n",
    "\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "#Scale both training set and test set (be careful do them independently!)\n",
    "scaler.fit(X_train)\n",
    "X_trainStandard = scaler.transform(X_train)\n",
    "X_testStandard = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Ridge() function has an alpha argument ( Î» , but with a different name!) that is used to tune the model. \n",
    "ridge2 = Ridge(alpha = 4)\n",
    "ridge2.fit(X_trainStandard, y_train)             # Fit a ridge regression on the training data\n",
    "pred2 = ridge2.predict(X_testStandard)           # Use this model to predict the test data\n",
    "print(pd.Series(ridge2.coef_, index = X.columns)) # Print coefficients\n",
    "print(mean_squared_error(y_test, pred2))          # Calculate the test MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge3 = Ridge(alpha = 10**10)\n",
    "ridge3.fit(X_trainStandard, y_train)             # Fit a ridge regression on the training data\n",
    "pred3 = ridge3.predict(X_testStandard)           # Use this model to predict the test data\n",
    "print(pd.Series(ridge3.coef_, index = X.columns)) # Print coefficients\n",
    "print(mean_squared_error(y_test, pred3))          # Calculate the test MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge4 = Ridge(alpha = 0)\n",
    "ridge4.fit(X_trainStandard, y_train)             # Fit a ridge regression on the training data\n",
    "pred = ridge4.predict(X_testStandard)            # Use this model to predict the test data\n",
    "print(pd.Series(ridge4.coef_, index = X.columns)) # Print coefficients\n",
    "print(mean_squared_error(y_test, pred))           # Calculate the test MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll generate an array of alpha values ranging from very big to very small, essentially covering the full range of scenarios \n",
    "#from the null model containing only the intercept, to the least squares fit\n",
    "alphas = 10**np.linspace(5,-2,100)*0.5\n",
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    ridge.set_params(alpha = a)\n",
    "    ridge.fit(X_trainStandard, y_train)\n",
    "    coefs.append(ridge.coef_)\n",
    "np.shape(coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "alphas = 10**np.linspace(5,-2,100)*0.5\n",
    "scoresCV = []\n",
    "for l in alphas:\n",
    "    RidgeReg = make_pipeline(preprocessing.StandardScaler(), Ridge(alpha=l))\n",
    "    scoreCV = cross_val_score(RidgeReg, X_train, y_train, scoring='neg_mean_squared_error',\n",
    "                             cv=KFold(n_splits=10, shuffle=True,\n",
    "                                            random_state=1))\n",
    "    scoresCV.append([l,-1*np.mean(scoreCV)])\n",
    "df = pd.DataFrame(scoresCV,columns=['Lambda','Validation Error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.sort_values(['Validation Error']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge5 = Ridge(alpha = 121.006413)\n",
    "ridge5.fit(X_trainStandard, y_train)\n",
    "mean_squared_error(y_test, ridge5.predict(X_testStandard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(ridge5.coef_, index = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso2 = Lasso(alpha=100,max_iter = 10000)\n",
    "lasso2.fit(X_trainStandard, y_train)             # Fit a Lasso regression on the training data\n",
    "pred2 = lasso2.predict(X_testStandard)           # Use this model to predict the test data\n",
    "print(pd.Series(lasso2.coef_, index = X.columns)) # Print coefficients\n",
    "print(mean_squared_error(y_test, pred2))          # Calculate the test MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso3 = Lasso(alpha=0.1,max_iter = 10000)\n",
    "lasso3.fit(X_trainStandard, y_train)             # Fit a Lasso regression on the training data\n",
    "pred3 = lasso3.predict(X_testStandard)           # Use this model to predict the test data\n",
    "print(pd.Series(lasso3.coef_, index=X.columns)) # Print coefficients\n",
    "print(mean_squared_error(y_test, pred3))          # Calculate the test MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = 10**np.linspace(1,-2,100)*0.5\n",
    "lasso = Lasso(max_iter = 10000)\n",
    "coefs = []\n",
    "for a in alphas:\n",
    "    lasso.set_params(alpha=a)\n",
    "    lasso.fit(X_trainStandard, y_train)\n",
    "    coefs.append(lasso.coef_)\n",
    "    \n",
    "ax = plt.gca()\n",
    "ax.plot(alphas*2, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "alphas = 10**np.linspace(1,-2,100)*0.5\n",
    "scoresCV = []\n",
    "for l in alphas:\n",
    "    lassoReg = make_pipeline(preprocessing.StandardScaler(), Lasso(alpha=l,max_iter=10000))\n",
    "    scoreCV = cross_val_score(lassoReg, X_train, y_train, scoring='neg_mean_squared_error',\n",
    "                             cv=KFold(n_splits=10, shuffle=True,\n",
    "                                            random_state=1))\n",
    "    scoresCV.append([l,-1*np.mean(scoreCV)])\n",
    "df = pd.DataFrame(scoresCV,columns=['Lambda','Validation Error'])\n",
    "print(df.sort_values(by='Validation Error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df.Lambda,df['Validation Error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha = 0.017556,max_iter=10000)\n",
    "lasso.fit(X_trainStandard, y_train)\n",
    "mean_squared_error(y_test, lasso.predict(X_testStandard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(lasso.coef_, index = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
